{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMO Prize 3 - OOP Baseline Submission (v5)\n",
    "\n",
    "This notebook demonstrates how to use the Object-Oriented Baseline with **Few-Shot Prompting**, **Majority Voting**, and **Parquet Submission**.\n",
    "\n",
    "## Setup Instructions\n",
    "1. **Add Utility Script**: Upload `src/kaggle_baseline.py` as a Dataset (e.g., named `aimo-pp3-source`).\n",
    "2. **Add Model**: Search for and add the model `Qwen/Qwen2.5-Math-7B-Instruct`.\n",
    "3. **Attach Competition Data**: Ensure the AIMO 3 competition data is attached.\n",
    "4. **Run**: Execute the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "print('--- DIAGNOSTIC INFO ---')\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "print('Contents of /kaggle/input/:')\n",
    "try:\n",
    "    for item in sorted(os.listdir('/kaggle/input')):\n",
    "        full_path = os.path.join('/kaggle/input', item)\n",
    "        if os.path.isdir(full_path):\n",
    "            print(f'  {item}/')\n",
    "            if any(x in item.lower() for x in ['competition', 'prize', 'module', 'qwen']):\n",
    "                try:\n",
    "                    for sub_item in sorted(os.listdir(full_path)):\n",
    "                        print(f'    - {sub_item}')\n",
    "                except Exception as e:\n",
    "                    print(f'    (Error listing {item}: {e})')\n",
    "        else:\n",
    "            print(f'  {item}')\n",
    "except Exception as e:\n",
    "    print(f'Error listing /kaggle/input/: {e}')\n",
    "\n",
    "print('\\nEnvironment variables:')\n",
    "for key, value in sorted(os.environ.items()):\n",
    "    if key.startswith('KAGGLE') or 'PATH' in key or 'HOME' in key:\n",
    "        print(f'{key}={value}')\n",
    "\n",
    "print('--- END DIAGNOSTIC INFO ---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INLINED KAGGLE BASELINE MODULE\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import io\n",
    "import contextlib\n",
    "import signal\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Dict, Any, List\n",
    "from collections import Counter\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration & Environment Handling\n",
    "# ==========================================\n",
    "class CompetitionConfig:\n",
    "    def __init__(self):\n",
    "        self.is_kaggle = os.path.exists(\"/kaggle/input\")\n",
    "        \n",
    "        if self.is_kaggle:\n",
    "            self.base_dir = \"/kaggle/input\"\n",
    "            # Kaggle에 추가할 Qwen 모델 경로 (예시)\n",
    "            self.model_path = \"/kaggle/input/qwen2-5-math-7b-instruct\" \n",
    "        else:\n",
    "            self.base_dir = \"./data\"\n",
    "            self.model_path = \"Qwen/Qwen2.5-Math-1.5B-Instruct\" # 로컬 테스트용 가벼운 모델\n",
    "\n",
    "        # 하이퍼파라미터 (Top-tier 커널 참고)\n",
    "        self.timeout_seconds = 10\n",
    "        self.n_repetitions = 16  # 한 문제당 16번 다르게 풀기 시도\n",
    "        self.temperature = 0.7   # 다양한 풀이 경로를 위한 높은 온도\n",
    "        self.max_tokens = 2048\n",
    "        self.gpu_memory_utilization = 0.95\n",
    "\n",
    "# ==========================================\n",
    "# 2. Code Execution Environment (Stateful)\n",
    "# ==========================================\n",
    "class CodeExecutor:\n",
    "    \"\"\"Thread-safe, optionally stateful Python executor.\"\"\"\n",
    "    def __init__(self, timeout: int = 5):\n",
    "        self.timeout = timeout\n",
    "        self.globals_dict = {} # 상태 유지를 위한 전역 변수 사전\n",
    "\n",
    "    def execute(self, code: str, reset_state: bool = True) -> str:\n",
    "        if reset_state:\n",
    "            self.globals_dict = {}\n",
    "            \n",
    "        output_buffer = io.StringIO()\n",
    "        \n",
    "        def timeout_handler(signum, frame):\n",
    "            raise TimeoutError(\"Execution timed out\")\n",
    "\n",
    "        use_timeout = False\n",
    "        if hasattr(signal, 'SIGALRM'):\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(self.timeout)\n",
    "                use_timeout = True\n",
    "            except ValueError:\n",
    "                pass # 백그라운드 스레드 무시\n",
    "        \n",
    "        try:\n",
    "            with contextlib.redirect_stdout(output_buffer):\n",
    "                # 기본적인 수학 라이브러리 자동 임포트\n",
    "                exec(\"import math\\nimport sympy\\nimport numpy as np\\n\", self.globals_dict)\n",
    "                exec(code, self.globals_dict)\n",
    "        except TimeoutError:\n",
    "            return \"Error: Execution timed out.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {type(e).__name__}: {str(e)}\"\n",
    "        finally:\n",
    "            if use_timeout and hasattr(signal, 'SIGALRM'):\n",
    "                signal.alarm(0)\n",
    "        \n",
    "        return output_buffer.getvalue().strip()\n",
    "\n",
    "# ==========================================\n",
    "# 3. Model Interface (vLLM Batched)\n",
    "# ==========================================\n",
    "class LLMInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def generate_batch(self, prompts: List[str]) -> List[str]:\n",
    "        pass\n",
    "\n",
    "class VLLMEngine(LLMInterface):\n",
    "    \"\"\"Real vLLM integration for high-throughput batch generation.\"\"\"\n",
    "    def __init__(self, config: CompetitionConfig):\n",
    "        try:\n",
    "            from vllm import LLM, SamplingParams\n",
    "            print(f\"Loading vLLM model from {config.model_path}...\")\n",
    "            # VRAM을 꽉 채워 쓰도록 설정\n",
    "            self.model = LLM(\n",
    "                model=config.model_path, \n",
    "                trust_remote_code=True,\n",
    "                tensor_parallel_size=1,\n",
    "                gpu_memory_utilization=config.gpu_memory_utilization,\n",
    "                max_model_len=4096, # 컨텍스트 길이 최적화\n",
    "                enforce_eager=True # Kaggle 환경 호환성\n",
    "            )\n",
    "            self.sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=0.9,\n",
    "                stop=[\"```\\n\", \"User:\", \"<|im_end|>\"]\n",
    "            )\n",
    "            self.is_mock = False\n",
    "            print(\"vLLM loaded successfully.\")\n",
    "        except ImportError:\n",
    "            print(\"⚠️ vLLM not installed. Falling back to MockLLM.\")\n",
    "            self.is_mock = True\n",
    "            \n",
    "    def generate_batch(self, prompts: List[str]) -> List[str]:\n",
    "        if self.is_mock:\n",
    "            import random\n",
    "            return [f\"The answer is \\\\boxed{{{random.randint(1, 100)}}}\" for _ in prompts]\n",
    "            \n",
    "        outputs = self.model.generate(prompts, self.sampling_params, use_tqdm=False)\n",
    "        return [output.outputs[0].text for output in outputs]\n",
    "\n",
    "# ==========================================\n",
    "# 4. Main Solver Logic\n",
    "# ==========================================\n",
    "class AIMSolver:\n",
    "    def __init__(self, config: CompetitionConfig, llm: LLMInterface):\n",
    "        self.config = config\n",
    "        self.llm = llm\n",
    "        \n",
    "    def format_prompt(self, problem: str) -> str:\n",
    "        \"\"\"Qwen Math Instruct Template\"\"\"\n",
    "        system = \"You are an expert mathematician. Solve the problem step-by-step. If you write Python code, enclose it in ```python\\n...\\n```. Always put your final answer inside \\\\boxed{}.\"\n",
    "        return f\"<|im_start|>system\\n{system}<|im_end|>\\n<|im_start|>user\\n{problem}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    def extract_answer(self, text: str) -> int:\n",
    "        match = re.search(r'\\\\boxed\\{([0-9,]+)\\}', text)\n",
    "        if match: \n",
    "            try:\n",
    "                return int(match.group(1).replace(',', '')) % 100000\n",
    "            except: pass\n",
    "        \n",
    "        # Fallback\n",
    "        match = re.search(r'final answer is\\s*([0-9,]+)', text, re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                return int(match.group(1).replace(',', '')) % 100000\n",
    "            except: pass\n",
    "        return -1\n",
    "\n",
    "    def solve(self, problem_text: str) -> int:\n",
    "        \"\"\"\n",
    "        Batched generation for Majority Voting.\n",
    "        1. 16개의 프롬프트를 한 번에 생성\n",
    "        2. 병렬로 응답 수집\n",
    "        3. 가장 많이 나온 답 선택\n",
    "        \"\"\"\n",
    "        # Create N identical prompts for sampling diverse paths (due to temp=0.7)\n",
    "        prompts = [self.format_prompt(problem_text)] * self.config.n_repetitions\n",
    "        \n",
    "        # Batch Generate (vLLM handles this extremely efficiently)\n",
    "        responses = self.llm.generate_batch(prompts)\n",
    "        \n",
    "        valid_answers = []\n",
    "        for resp in responses:\n",
    "            ans = self.extract_answer(resp)\n",
    "            if ans >= 0:\n",
    "                valid_answers.append(ans)\n",
    "                \n",
    "        if not valid_answers:\n",
    "            print(\"No valid answers found. Returning 0.\")\n",
    "            return 0\n",
    "            \n",
    "        # Majority Vote\n",
    "        counts = Counter(valid_answers)\n",
    "        most_common, count = counts.most_common(1)[0]\n",
    "        print(f\"Votes: {dict(counts)} -> Selected: {most_common}\")\n",
    "        return most_common\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configure Environment & Model\n",
    "config = CompetitionConfig()\n",
    "config.n_repetitions = 16 \n",
    "\n",
    "print(f\"Environment: {'Kaggle' if config.is_kaggle else 'Local'}\")\n",
    "\n",
    "if config.is_kaggle and os.path.exists(config.model_path):\n",
    "    print(f\"Loading real VLLM Engine from {config.model_path}...\")\n",
    "    llm = VLLMEngine(config)\n",
    "else:\n",
    "    print(\"Using MockLLM for local testing or if model path not found.\")\n",
    "    llm = MockLLM()\n",
    "\n",
    "solver = AIMSolver(config, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Loop (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. AIMO 3 API Setup (Inference Server Pattern)\n",
    "# --------------------------------------------------------------------------------\n",
    "aimo_server_mod = None\n",
    "\n",
    "api_files = glob.glob('/kaggle/input/**/aimo_3_inference_server.py', recursive=True)\n",
    "if not api_files:\n",
    "    api_files = glob.glob('data/**/aimo_3_inference_server.py', recursive=True)\n",
    "\n",
    "if api_files:\n",
    "    api_path = os.path.dirname(api_files[0])\n",
    "    if os.path.basename(api_path) == 'kaggle_evaluation':\n",
    "        parent_dir = os.path.dirname(api_path)\n",
    "        if parent_dir not in sys.path: sys.path.append(parent_dir)\n",
    "    elif api_path not in sys.path: sys.path.append(api_path)\n",
    "\n",
    "    try:\n",
    "        import aimo_3_inference_server as aimo_server_mod\n",
    "        print('✅ Imported aimo_3_inference_server.')\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from kaggle_evaluation import aimo_3_inference_server as aimo_server_mod\n",
    "            print('✅ Imported aimo_3_inference_server from package.')\n",
    "        except ImportError as e:\n",
    "            print(f'❌ Failed to import API: {e}')\n",
    "\n",
    "found_test_csv = glob.glob('/kaggle/input/**/test.csv', recursive=True)\n",
    "actual_test_csv = found_test_csv[0] if found_test_csv else '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv'\n",
    "\n",
    "def predict(*args, **kwargs):\n",
    "    try:\n",
    "        input_data = args[0] if args else None\n",
    "        problem_text = 'What is 0+0?'\n",
    "        if hasattr(input_data, 'columns') and 'problem' in input_data.columns:\n",
    "            try:\n",
    "                problem_text = str(input_data['problem'][0])\n",
    "            except:\n",
    "                problem_text = str(input_data.iloc[0]['problem'])\n",
    "        elif isinstance(input_data, str):\n",
    "            problem_text = input_data\n",
    "        \n",
    "        answer = solver.solve(problem_text)\n",
    "        return pd.DataFrame({'answer': [answer]})\n",
    "    except Exception as e:\n",
    "        print(f'Predict Error: {e}')\n",
    "        return pd.DataFrame({'answer': [0]})\n",
    "\n",
    "if aimo_server_mod:\n",
    "    try:\n",
    "        import aimo_3_gateway\n",
    "        old_gen = aimo_3_gateway.AIMO3Gateway.generate_data_batches\n",
    "        def patched_gen(self):\n",
    "            for data_batch, row_ids in old_gen(self):\n",
    "                yield (data_batch,), row_ids\n",
    "        aimo_3_gateway.AIMO3Gateway.generate_data_batches = patched_gen\n",
    "        aimo_3_gateway.AIMO3Gateway.target_column_name = 'answer'\n",
    "        aimo_3_gateway.AIMO3Gateway.row_id_column_name = 'id'\n",
    "        print('✅ Applied Gateway monkey-patches.')\n",
    "    except Exception as e:\n",
    "        print(f'⚠️ Failed to patch Gateway: {e}')\n",
    "\n",
    "    server = aimo_server_mod.AIMO3InferenceServer(predict)\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        server.serve()\n",
    "    else:\n",
    "        try:\n",
    "            server.run_local_gateway(data_paths=(actual_test_csv,))\n",
    "            print('✅ Local Gateway finished.')\n",
    "        except Exception as e:\n",
    "            print(f'❌ Local Gateway failed: {e}')\n",
    "            pd.DataFrame({'id': ['test'], 'answer': [0]}).to_parquet('submission.parquet', index=False)\n",
    "else:\n",
    "    pd.DataFrame({'id': ['test'], 'answer': [0]}).to_parquet('submission.parquet', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}