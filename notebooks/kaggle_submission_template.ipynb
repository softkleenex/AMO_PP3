{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print('Checking for vLLM...')\n",
    "try:\n",
    "    import vllm\n",
    "    print('vLLM is already installed.')\n",
    "except ImportError:\n",
    "    print('vLLM not found. Installing from offline wheels...')\n",
    "    # Using ermecan/aimo3-dependency-dataset which contains vllm wheels\n",
    "    wheel_dir = '/kaggle/input/aimo3-dependency-dataset'\n",
    "    if os.path.exists(wheel_dir):\n",
    "        print(f'Found wheel directory: {wheel_dir}')\n",
    "        try:\n",
    "            # The exact directory structure within the dataset might vary,\n",
    "            # often it's directly in the root or in a 'wheels' subfolder.\n",
    "            # Let's search for the .whl files directory.\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-index', '--find-links', wheel_dir, 'vllm'], check=True)\n",
    "            print('Successfully installed vLLM offline.')\n",
    "        except Exception as e:\n",
    "            print(f'Error installing vLLM: {e}')\n",
    "    else:\n",
    "        print(f'ERROR: Offline wheel directory {wheel_dir} not found. Please attach the dataset ermecan/aimo3-dependency-dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMO Prize 3 - OOP Baseline Submission (v5)\n",
    "\n",
    "This notebook demonstrates how to use the Object-Oriented Baseline with **Few-Shot Prompting**, **Majority Voting**, and **Parquet Submission**.\n",
    "\n",
    "## Setup Instructions\n",
    "1. **Add Utility Script**: Upload `src/kaggle_baseline.py` as a Dataset (e.g., named `aimo-pp3-source`).\n",
    "2. **Add Model**: Search for and add the model `Qwen/Qwen2.5-Math-7B-Instruct`.\n",
    "3. **Attach Competition Data**: Ensure the AIMO 3 competition data is attached.\n",
    "4. **Run**: Execute the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "print('--- DIAGNOSTIC INFO ---')\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "print('Contents of /kaggle/input/:')\n",
    "try:\n",
    "    for item in sorted(os.listdir('/kaggle/input')):\n",
    "        full_path = os.path.join('/kaggle/input', item)\n",
    "        if os.path.isdir(full_path):\n",
    "            print(f'  {item}/')\n",
    "            if any(x in item.lower() for x in ['competition', 'prize', 'module', 'qwen']):\n",
    "                try:\n",
    "                    for sub_item in sorted(os.listdir(full_path)):\n",
    "                        print(f'    - {sub_item}')\n",
    "                except Exception as e:\n",
    "                    print(f'    (Error listing {item}: {e})')\n",
    "        else:\n",
    "            print(f'  {item}')\n",
    "except Exception as e:\n",
    "    print(f'Error listing /kaggle/input/: {e}')\n",
    "\n",
    "print('\\nEnvironment variables:')\n",
    "for key, value in sorted(os.environ.items()):\n",
    "    if key.startswith('KAGGLE') or 'PATH' in key or 'HOME' in key:\n",
    "        print(f'{key}={value}')\n",
    "\n",
    "print('--- END DIAGNOSTIC INFO ---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INLINED KAGGLE BASELINE MODULE\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import io\n",
    "import contextlib\n",
    "import signal\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Dict, Any, List\n",
    "from collections import Counter\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration & Environment Handling\n",
    "# ==========================================\n",
    "class CompetitionConfig:\n",
    "    def __init__(self):\n",
    "        self.is_kaggle = os.path.exists(\"/kaggle/input\")\n",
    "        \n",
    "        if self.is_kaggle:\n",
    "            self.base_dir = \"/kaggle/input\"\n",
    "            # Kaggle\uc5d0 \ucd94\uac00\ud560 Qwen \ubaa8\ub378 \uacbd\ub85c (\uc608\uc2dc)\n",
    "            self.model_path = \"/kaggle/input/qwen2-5-math-7b-instruct\" \n",
    "        else:\n",
    "            self.base_dir = \"./data\"\n",
    "            self.model_path = \"Qwen/Qwen2.5-Math-1.5B-Instruct\" # \ub85c\uceec \ud14c\uc2a4\ud2b8\uc6a9 \uac00\ubcbc\uc6b4 \ubaa8\ub378\n",
    "\n",
    "        # \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 (Top-tier \ucee4\ub110 \ucc38\uace0)\n",
    "        self.timeout_seconds = 10\n",
    "        self.n_repetitions = 16  # \ud55c \ubb38\uc81c\ub2f9 16\ubc88 \ub2e4\ub974\uac8c \ud480\uae30 \uc2dc\ub3c4\n",
    "        self.temperature = 0.7   # \ub2e4\uc591\ud55c \ud480\uc774 \uacbd\ub85c\ub97c \uc704\ud55c \ub192\uc740 \uc628\ub3c4\n",
    "        self.max_tokens = 2048\n",
    "        self.gpu_memory_utilization = 0.95\n",
    "\n",
    "# ==========================================\n",
    "# 2. Code Execution Environment (Stateful)\n",
    "# ==========================================\n",
    "class CodeExecutor:\n",
    "    \"\"\"Thread-safe, optionally stateful Python executor.\"\"\"\n",
    "    def __init__(self, timeout: int = 5):\n",
    "        self.timeout = timeout\n",
    "        self.globals_dict = {} # \uc0c1\ud0dc \uc720\uc9c0\ub97c \uc704\ud55c \uc804\uc5ed \ubcc0\uc218 \uc0ac\uc804\n",
    "\n",
    "    def execute(self, code: str, reset_state: bool = True) -> str:\n",
    "        if reset_state:\n",
    "            self.globals_dict = {}\n",
    "            \n",
    "        output_buffer = io.StringIO()\n",
    "        \n",
    "        def timeout_handler(signum, frame):\n",
    "            raise TimeoutError(\"Execution timed out\")\n",
    "\n",
    "        use_timeout = False\n",
    "        if hasattr(signal, 'SIGALRM'):\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(self.timeout)\n",
    "                use_timeout = True\n",
    "            except ValueError:\n",
    "                pass # \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc2a4\ub808\ub4dc \ubb34\uc2dc\n",
    "        \n",
    "        try:\n",
    "            with contextlib.redirect_stdout(output_buffer):\n",
    "                # \uae30\ubcf8\uc801\uc778 \uc218\ud559 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc790\ub3d9 \uc784\ud3ec\ud2b8\n",
    "                exec(\"import math\\nimport sympy\\nimport numpy as np\\n\", self.globals_dict)\n",
    "                exec(code, self.globals_dict)\n",
    "        except TimeoutError:\n",
    "            return \"Error: Execution timed out.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {type(e).__name__}: {str(e)}\"\n",
    "        finally:\n",
    "            if use_timeout and hasattr(signal, 'SIGALRM'):\n",
    "                signal.alarm(0)\n",
    "        \n",
    "        return output_buffer.getvalue().strip()\n",
    "\n",
    "# ==========================================\n",
    "# 3. Model Interface (vLLM Batched)\n",
    "# ==========================================\n",
    "class LLMInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def generate_batch(self, prompts: List[str]) -> List[str]:\n",
    "        pass\n",
    "\n",
    "class VLLMEngine(LLMInterface):\n",
    "    \"\"\"Real vLLM integration for high-throughput batch generation.\"\"\"\n",
    "    def __init__(self, config: CompetitionConfig):\n",
    "        try:\n",
    "            from vllm import LLM, SamplingParams\n",
    "            print(f\"Loading vLLM model from {config.model_path}...\")\n",
    "            # VRAM\uc744 \uaf49 \ucc44\uc6cc \uc4f0\ub3c4\ub85d \uc124\uc815\n",
    "            self.model = LLM(\n",
    "                model=config.model_path, \n",
    "                trust_remote_code=True,\n",
    "                tensor_parallel_size=1,\n",
    "                gpu_memory_utilization=config.gpu_memory_utilization,\n",
    "                max_model_len=4096, # \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774 \ucd5c\uc801\ud654\n",
    "                enforce_eager=True # Kaggle \ud658\uacbd \ud638\ud658\uc131\n",
    "            )\n",
    "            self.sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=0.9,\n",
    "                stop=[\"```\\n\", \"User:\", \"<|im_end|>\"]\n",
    "            )\n",
    "            self.is_mock = False\n",
    "            print(\"vLLM loaded successfully.\")\n",
    "        except ImportError:\n",
    "            print(\"\u26a0\ufe0f vLLM not installed. Falling back to MockLLM.\")\n",
    "            self.is_mock = True\n",
    "            \n",
    "    def generate_batch(self, prompts: List[str]) -> List[str]:\n",
    "        if self.is_mock:\n",
    "            import random\n",
    "            return [f\"The answer is \\\\boxed{{{random.randint(1, 100)}}}\" for _ in prompts]\n",
    "            \n",
    "        outputs = self.model.generate(prompts, self.sampling_params, use_tqdm=False)\n",
    "        return [output.outputs[0].text for output in outputs]\n",
    "\n",
    "# ==========================================\n",
    "# 4. Main Solver Logic\n",
    "# ==========================================\n",
    "class AIMSolver:\n",
    "    def __init__(self, config: CompetitionConfig, llm: LLMInterface):\n",
    "        self.config = config\n",
    "        self.llm = llm\n",
    "        \n",
    "    def format_prompt(self, problem: str) -> str:\n",
    "        \"\"\"Qwen Math Instruct Template\"\"\"\n",
    "        system = \"You are an expert mathematician. Solve the problem step-by-step. If you write Python code, enclose it in ```python\\n...\\n```. Always put your final answer inside \\\\boxed{}.\"\n",
    "        return f\"<|im_start|>system\\n{system}<|im_end|>\\n<|im_start|>user\\n{problem}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    def extract_answer(self, text: str) -> int:\n",
    "        match = re.search(r'\\\\boxed\\{([0-9,]+)\\}', text)\n",
    "        if match: \n",
    "            try:\n",
    "                return int(match.group(1).replace(',', '')) % 100000\n",
    "            except: pass\n",
    "        \n",
    "        # Fallback\n",
    "        match = re.search(r'final answer is\\s*([0-9,]+)', text, re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                return int(match.group(1).replace(',', '')) % 100000\n",
    "            except: pass\n",
    "        return -1\n",
    "\n",
    "    def solve(self, problem_text: str) -> int:\n",
    "        \"\"\"\n",
    "        Batched generation for Majority Voting.\n",
    "        1. 16\uac1c\uc758 \ud504\ub86c\ud504\ud2b8\ub97c \ud55c \ubc88\uc5d0 \uc0dd\uc131\n",
    "        2. \ubcd1\ub82c\ub85c \uc751\ub2f5 \uc218\uc9d1\n",
    "        3. \uac00\uc7a5 \ub9ce\uc774 \ub098\uc628 \ub2f5 \uc120\ud0dd\n",
    "        \"\"\"\n",
    "        # Create N identical prompts for sampling diverse paths (due to temp=0.7)\n",
    "        prompts = [self.format_prompt(problem_text)] * self.config.n_repetitions\n",
    "        \n",
    "        # Batch Generate (vLLM handles this extremely efficiently)\n",
    "        responses = self.llm.generate_batch(prompts)\n",
    "        \n",
    "        valid_answers = []\n",
    "        for resp in responses:\n",
    "            ans = self.extract_answer(resp)\n",
    "            if ans >= 0:\n",
    "                valid_answers.append(ans)\n",
    "                \n",
    "        if not valid_answers:\n",
    "            print(\"No valid answers found. Returning 0.\")\n",
    "            return 0\n",
    "            \n",
    "        # Majority Vote\n",
    "        counts = Counter(valid_answers)\n",
    "        most_common, count = counts.most_common(1)[0]\n",
    "        print(f\"Votes: {dict(counts)} -> Selected: {most_common}\")\n",
    "        return most_common\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configure Environment & Model\n",
    "config = CompetitionConfig()\n",
    "config.n_repetitions = 16 \n",
    "\n",
    "print(f\"Environment: {'Kaggle' if config.is_kaggle else 'Local'}\")\n",
    "\n",
    "if config.is_kaggle and os.path.exists(config.model_path):\n",
    "    print(f\"Loading real VLLM Engine from {config.model_path}...\")\n",
    "    llm = VLLMEngine(config)\n",
    "else:\n",
    "    print(\"Using MockLLM for local testing or if model path not found.\")\n",
    "    llm = MockLLM()\n",
    "\n",
    "solver = AIMSolver(config, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Loop (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. AIMO 3 API Setup (Inference Server Pattern)\n",
    "# --------------------------------------------------------------------------------\n",
    "aimo_server_mod = None\n",
    "\n",
    "api_files = glob.glob('/kaggle/input/**/aimo_3_inference_server.py', recursive=True)\n",
    "if not api_files:\n",
    "    api_files = glob.glob('data/**/aimo_3_inference_server.py', recursive=True)\n",
    "\n",
    "if api_files:\n",
    "    api_path = os.path.dirname(api_files[0])\n",
    "    if os.path.basename(api_path) == 'kaggle_evaluation':\n",
    "        parent_dir = os.path.dirname(api_path)\n",
    "        if parent_dir not in sys.path: sys.path.append(parent_dir)\n",
    "    elif api_path not in sys.path: sys.path.append(api_path)\n",
    "\n",
    "    try:\n",
    "        import aimo_3_inference_server as aimo_server_mod\n",
    "        print('\u2705 Imported aimo_3_inference_server.')\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from kaggle_evaluation import aimo_3_inference_server as aimo_server_mod\n",
    "            print('\u2705 Imported aimo_3_inference_server from package.')\n",
    "        except ImportError as e:\n",
    "            print(f'\u274c Failed to import API: {e}')\n",
    "\n",
    "found_test_csv = glob.glob('/kaggle/input/**/test.csv', recursive=True)\n",
    "actual_test_csv = found_test_csv[0] if found_test_csv else '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv'\n",
    "\n",
    "def predict(*args, **kwargs):\n",
    "    try:\n",
    "        input_data = args[0] if args else None\n",
    "        problem_text = 'What is 0+0?'\n",
    "        if hasattr(input_data, 'columns') and 'problem' in input_data.columns:\n",
    "            try:\n",
    "                problem_text = str(input_data['problem'][0])\n",
    "            except:\n",
    "                problem_text = str(input_data.iloc[0]['problem'])\n",
    "        elif isinstance(input_data, str):\n",
    "            problem_text = input_data\n",
    "        \n",
    "        answer = solver.solve(problem_text)\n",
    "        return pd.DataFrame({'answer': [answer]})\n",
    "    except Exception as e:\n",
    "        print(f'Predict Error: {e}')\n",
    "        return pd.DataFrame({'answer': [0]})\n",
    "\n",
    "if aimo_server_mod:\n",
    "    try:\n",
    "        import aimo_3_gateway\n",
    "        old_gen = aimo_3_gateway.AIMO3Gateway.generate_data_batches\n",
    "        def patched_gen(self):\n",
    "            for data_batch, row_ids in old_gen(self):\n",
    "                yield (data_batch,), row_ids\n",
    "        aimo_3_gateway.AIMO3Gateway.generate_data_batches = patched_gen\n",
    "        aimo_3_gateway.AIMO3Gateway.target_column_name = 'answer'\n",
    "        aimo_3_gateway.AIMO3Gateway.row_id_column_name = 'id'\n",
    "        print('\u2705 Applied Gateway monkey-patches.')\n",
    "    except Exception as e:\n",
    "        print(f'\u26a0\ufe0f Failed to patch Gateway: {e}')\n",
    "\n",
    "    server = aimo_server_mod.AIMO3InferenceServer(predict)\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        server.serve()\n",
    "    else:\n",
    "        try:\n",
    "            server.run_local_gateway(data_paths=(actual_test_csv,))\n",
    "            print('\u2705 Local Gateway finished.')\n",
    "        except Exception as e:\n",
    "            print(f'\u274c Local Gateway failed: {e}')\n",
    "            pd.DataFrame({'id': ['test'], 'answer': [0]}).to_parquet('submission.parquet', index=False)\n",
    "else:\n",
    "    pd.DataFrame({'id': ['test'], 'answer': [0]}).to_parquet('submission.parquet', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}