{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMO Progress Prize 3 - Submission V2 (Code Interpreter)
",
    "
",
    "This notebook implements the **Tool-Integrated Reasoning (TIR)** pipeline.
",
    "It generates Python code (simulated) and executes it to solve problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd
",
    "import sys
",
    "import os
",
    "import importlib
",
    "
",
    "# Setup paths
",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))
",
    "try:
",
    "    import solver
",
    "    importlib.reload(solver) # Force reload if logic changed
",
    "    from solver import solve
",
    "except ImportError:
",
    "    sys.path.append(os.path.join(os.getcwd(), 'src'))
",
    "    import solver
",
    "    importlib.reload(solver)
",
    "    from solver import solve
",
    "
",
    "print("Solver imported.")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Environment Definition (Same as before)
",
    "class MockAIMOEnv:
",
    "    def __init__(self, reference_path='../data/reference.csv'):
",
    "        self.reference_df = pd.read_csv(reference_path)
",
    "        self.predictions = []
",
    "    
",
    "    def iter_test(self):
",
    "        for i in range(len(self.reference_df)):
",
    "            row = self.reference_df.iloc[[i]]
",
    "            test_df = row[['id', 'problem']].copy()
",
    "            sample_submission_df = row[['id']].copy()
",
    "            sample_submission_df['answer'] = 0
",
    "            yield test_df, sample_submission_df
",
    "            
",
    "    def predict(self, submission_df):
",
    "        self.predictions.append(submission_df.iloc[0].to_dict())
",
    "        
",
    "    def score(self):
",
    "        pred_df = pd.DataFrame(self.predictions)
",
    "        merged = pred_df.merge(self.reference_df, on='id', suffixes=('_pred', '_true'))
",
    "        merged['correct'] = (merged['answer_pred'].astype(int) == merged['answer_true'].astype(int))
",
    "        return merged['correct'].mean(), merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution
",
    "try:
",
    "    env = MockAIMOEnv()
",
    "    print("Loaded reference data.")
",
    "except FileNotFoundError:
",
    "    print("Data not found, skipping.")
",
    "    env = None
",
    "
",
    "if env:
",
    "    for (test_df, sample_submission) in env.iter_test():
",
    "        problem_text = test_df['problem'].values[0]
",
    "        
",
    "        # Use the code-execution solver
",
    "        prediction = solve(problem_text)
",
    "        
",
    "        sample_submission['answer'] = prediction
",
    "        env.predict(sample_submission)
",
    "
",
    "    score, details = env.score()
",
    "    print(f"Final Accuracy (Mock LLM): {score:.4f}")
",
    "    
",
    "    # Show the ones we got right (likely just the hardcoded ones)
",
    "    print("Correctly Solved:")
",
    "    print(details[details['correct']][['id', 'problem', 'answer_pred', 'answer_true']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}