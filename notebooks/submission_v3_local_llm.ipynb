{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMO Progress Prize 3 - Submission V3 (Local LLM)
",
    "
",
    "This notebook integrates a **real local LLM** (`Qwen2.5-Math-1.5B-Instruct`) into the Tool-Integrated Reasoning (TIR) pipeline.
",
    "
",
    "**Requirements:**
",
    "- `pip install torch transformers accelerate`
",
    "- A decent CPU or GPU (Apple Silicon MPS supported)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running on Colab/Kaggle
",
    "!pip install -q torch transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch
",
    "from transformers import AutoModelForCausalLM, AutoTokenizer
",
    "import sys
",
    "import os
",
    "import importlib
",
    "
",
    "# Add src to path
",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))
",
    "import solver
",
    "import utils
",
    "importlib.reload(solver)
",
    "importlib.reload(utils)
",
    "from utils import PythonREPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model
",
    "We use `Qwen/Qwen2.5-Math-1.5B-Instruct` for its balance of speed and math capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = "Qwen/Qwen2.5-Math-1.5B-Instruct"
",
    "
",
    "try:
",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
",
    "    model = AutoModelForCausalLM.from_pretrained(
",
    "        MODEL_NAME, 
",
    "        device_map="auto", 
",
    "        trust_remote_code=True,
",
    "        torch_dtype=torch.float16
",
    "    )
",
    "    print(f"Model {MODEL_NAME} loaded successfully.")
",
    "except Exception as e:
",
    "    print(f"Error loading model: {e}")
",
    "    print("Using Mock LLM for fallback.")
",
    "    model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Generator Function
",
    "Replace the mock generator with the real model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_with_llm(problem_text: str) -> str:
",
    "    if model is None:
",
    "        return solver.mock_llm_generate_code(problem_text)
",
    "
",
    "    # Construct Prompt
",
    "    messages = [
",
    "        {"role": "system", "content": "You are a helpful assistant who solves math problems by writing Python code. Output only the code wrapped in ```python ... ``` blocks."},
",
    "        {"role": "user", "content": f"Problem: {problem_text}

Write a Python script to solve this. Print the final answer at the end."}
",
    "    ]
",
    "    text = tokenizer.apply_chat_template(
",
    "        messages,
",
    "        tokenize=False,
",
    "        add_generation_prompt=True
",
    "    )
",
    "    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
",
    "
",
    "    generated_ids = model.generate(
",
    "        **model_inputs,
",
    "        max_new_tokens=512
",
    "    )
",
    "    generated_ids = [
",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
",
    "    ]
",
    "
",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
",
    "    
",
    "    # Extract code block
",
    "    import re
",
    "    code_match = re.search(r'```python
(.*?)
```', response, re.DOTALL)
",
    "    if code_match:
",
    "        return code_match.group(1)
",
    "    
",
    "    # Fallback: try to find any code-like structure or return raw
",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey-patch the solver to use our new generator
",
    "solver.mock_llm_generate_code = generate_code_with_llm
",
    "
",
    "# Re-use MockEnv from V2 logic
",
    "class MockAIMOEnv:
",
    "    def __init__(self, reference_path='../data/reference.csv'):
",
    "        self.reference_df = pd.read_csv(reference_path)
",
    "        self.predictions = []
",
    "    
",
    "    def iter_test(self):
",
    "        for i in range(min(5, len(self.reference_df))): # Run first 5 for speed
",
    "            row = self.reference_df.iloc[[i]]
",
    "            test_df = row[['id', 'problem']].copy()
",
    "            sample_submission_df = row[['id']].copy()
",
    "            sample_submission_df['answer'] = 0
",
    "            yield test_df, sample_submission_df
",
    "            
",
    "    def predict(self, submission_df):
",
    "        self.predictions.append(submission_df.iloc[0].to_dict())
",
    "        
",
    "    def score(self):
",
    "        pred_df = pd.DataFrame(self.predictions)
",
    "        merged = pred_df.merge(self.reference_df, on='id', suffixes=('_pred', '_true'))
",
    "        merged['correct'] = (merged['answer_pred'].astype(int) == merged['answer_true'].astype(int))
",
    "        return merged['correct'].mean(), merged
",
    "
",
    "env = MockAIMOEnv()
",
    "
",
    "print("Starting inference loop...")
",
    "for (test_df, sample_submission) in env.iter_test():
",
    "    problem_text = test_df['problem'].values[0]
",
    "    print(f"Solving problem {test_df['id'].values[0]}...")
",
    "    
",
    "    prediction = solver.solve(problem_text)
",
    "    
",
    "    sample_submission['answer'] = prediction
",
    "    env.predict(sample_submission)
",
    "    print(f"  -> Prediction: {prediction}")
",
    "
",
    "score, details = env.score()
",
    "print(f"Final Accuracy (Qwen-1.5B): {score:.4f}")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}