{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMO Progress Prize 3 - Submission V1
",
    "
",
    "This notebook implements a complete local validation pipeline using `reference.csv`.
",
    "It mimics the competition's API structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd
",
    "import sys
",
    "import os
",
    "
",
    "# Add src to path for local imports
",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))
",
    "try:
",
    "    from solver import solve
",
    "except ImportError:
",
    "    # Fallback if running from a different root
",
    "    sys.path.append(os.path.join(os.getcwd(), 'src'))
",
    "    from solver import solve
",
    "
",
    "print("Solver imported successfully.")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Local Mock Environment
",
    "This class simulates the `aimo` module behavior using `reference.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockAIMOEnv:
",
    "    def __init__(self, reference_path='../data/reference.csv'):
",
    "        self.reference_df = pd.read_csv(reference_path)
",
    "        self.predictions = []
",
    "        self.index = 0
",
    "    
",
    "    def iter_test(self):
",
    "        """Yields (test_df, sample_submission_df) tuples."""
",
    "        for i in range(len(self.reference_df)):
",
    "            row = self.reference_df.iloc[[i]]
",
    "            test_df = row[['id', 'problem']].copy()
",
    "            sample_submission_df = row[['id']].copy()
",
    "            sample_submission_df['answer'] = 0
",
    "            yield test_df, sample_submission_df
",
    "            
",
    "    def predict(self, submission_df):
",
    "        """Receives the submission dataframe."""
",
    "        self.predictions.append(submission_df.iloc[0].to_dict())
",
    "        
",
    "    def score(self):
",
    "        """Calculates accuracy based on collected predictions."""
",
    "        pred_df = pd.DataFrame(self.predictions)
",
    "        merged = pred_df.merge(self.reference_df, on='id', suffixes=('_pred', '_true'))
",
    "        
",
    "        # The answer might be string or int, ensure comparison is robust
",
    "        merged['correct'] = (merged['answer_pred'].astype(int) == merged['answer_true'].astype(int))
",
    "        accuracy = merged['correct'].mean()
",
    "        return accuracy, merged
",
    "
",
    "print("Mock Environment defined.")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Main Execution Loop
",
    "Run the solver on the reference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mock Env
",
    "try:
",
    "    env = MockAIMOEnv()
",
    "    print("Loaded reference data.")
",
    "except FileNotFoundError:
",
    "    print("Warning: reference.csv not found in ../data/. Ensure you are running from 'notebooks/' directory.")
",
    "    # Fallback for creating a dummy env if file is missing (e.g. CI)
",
    "    class DummyEnv:
",
    "        def iter_test(self): return []
",
    "        def score(self): return 0.0, pd.DataFrame()
",
    "    env = DummyEnv()
",
    "
",
    "# Run Loop
",
    "import time
",
    "start_time = time.time()
",
    "
",
    "for (test_df, sample_submission) in env.iter_test():
",
    "    problem_text = test_df['problem'].values[0]
",
    "    
",
    "    # --- SOLVER LOGIC ---
",
    "    try:
",
    "        prediction = solve(problem_text)
",
    "    except Exception as e:
",
    "        print(f"Error solving {test_df['id'].values[0]}: {e}")
",
    "        prediction = 0
",
    "    # --------------------
",
    "    
",
    "    sample_submission['answer'] = prediction
",
    "    env.predict(sample_submission)
",
    "
",
    "end_time = time.time()
",
    "duration = end_time - start_time
",
    "print(f"Completed in {duration:.2f} seconds.")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(env, MockAIMOEnv):
",
    "    score, details = env.score()
",
    "    print(f"Final Accuracy: {score:.4f}")
",
    "    print("
First 5 predictions:")
",
    "    print(details[['id', 'problem', 'answer_pred', 'answer_true', 'correct']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}